import datetime as dt
import json
import os
import time
from pathlib import Path

import ccxt
import pandas as pd
from ccxt.base.exchange import Exchange
from dotenv import load_dotenv
from loguru import logger
from tqdm import tqdm

# ---------------- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ----------------

load_dotenv()

DATA_DIR = Path(os.getenv("DATA_DIR", "./data"))
RAW_DIR = DATA_DIR / "raw"
RAW_DIR.mkdir(parents=True, exist_ok=True)

TIMEFRAMES = {"1d": "1d", "1h": "1h", "5m": "5m"}

# ---------------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------------


def get_exchange(name: str) -> Exchange:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –±–∏—Ä–∂–∏ ccxt –ø–æ –∏–º–µ–Ω–∏."""
    if not hasattr(ccxt, name):
        raise AttributeError(f"Exchange {name} not found in ccxt module")
    cls = getattr(ccxt, name)
    return cls({"enableRateLimit": True})


def to_utc_datetime(value: str | dt.datetime) -> dt.datetime:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É '2023-12-23' –∏–ª–∏ datetime –±–µ–∑ tz -> datetime —Å UTC"""
    if isinstance(value, dt.datetime):
        if value.tzinfo is None:
            return value.replace(tzinfo=dt.UTC)
        return value.astimezone(dt.UTC)
    elif isinstance(value, str):
        return dt.datetime.strptime(value, "%Y-%m-%d").replace(tzinfo=dt.UTC)
    else:
        raise TypeError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø: {type(value)} (–æ–∂–∏–¥–∞–ª–∞—Å—å str –∏–ª–∏ datetime)")


# ---------------- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö ----------------


def qc_summary(df: pd.DataFrame, tf: str) -> float:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö: –Ω–µ—Ç –ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤.
    """
    if df.empty:
        logger.warning("‚ö†Ô∏è DataFrame –ø—É—Å—Ç ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
        return 100.0

    freq_map = {"1d": "1D", "1h": "1H", "5m": "5T"}
    if tf not in freq_map:
        logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º '{tf}', QC –ø—Ä–æ–ø—É—â–µ–Ω.")
        return 0.0

    expected = pd.date_range(df.index.min(), df.index.max(), freq=freq_map[tf], tz="UTC")
    missing = expected.difference(df.index)

    pct_missing = len(missing) / len(expected) * 100 if len(expected) else 0
    if pct_missing > 0:
        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {len(missing)} —Å–≤–µ—á–µ–π ({pct_missing:.2f}%) –¥–ª—è {tf}")
    else:
        logger.info(f"‚úÖ –í—Å–µ —Å–≤–µ—á–∏ –Ω–∞ –º–µ—Å—Ç–µ –¥–ª—è {tf}")
    return pct_missing


# ---------------- –ó–∞–ø–∏—Å—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ----------------


def write_metadata(
    parquet_path: Path,
    exchange: str,
    symbol: str,
    tf: str,
    start: dt.datetime,
    end: dt.datetime,
    rows: int,
    missing_pct: float,
) -> None:
    """–°–æ–∑–¥–∞—ë—Ç JSON-—Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ —Ä—è–¥–æ–º —Å parquet."""
    meta = {
        "exchange": exchange,
        "symbol": symbol,
        "timeframe": tf,
        "rows": rows,
        "pct_missing": round(missing_pct, 3),
        "from": str(start),
        "to": str(end),
        "utc_saved": dt.datetime.now(dt.UTC).isoformat(),
    }
    json_path = parquet_path.with_suffix(".json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    logger.info(f"üßæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã ‚Üí {json_path.name}")


# ---------------- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ ----------------


def fetch_ohlcv(
    exchange_name: str,
    symbol: str,
    timeframe: str,
    since: str | dt.datetime,
    until: str | dt.datetime,
    limit: int = 1000,
) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–≤–µ—á–∏ OHLCV —á–µ—Ä–µ–∑ ccxt –∫—É—Å–∫–∞–º–∏ –ø–æ limit.
    –î–æ–±–∞–≤–ª–µ–Ω tqdm-–ø—Ä–æ–≥—Ä–µ—Å—Å –∏ autosave –∫–∞–∂–¥—ã–µ 10 000 —Å—Ç—Ä–æ–∫.
    """

    since_dt = to_utc_datetime(since)
    until_dt = to_utc_datetime(until)

    if since_dt >= until_dt:
        raise ValueError(f"'since' ({since_dt}) –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–Ω—å—à–µ 'until' ({until_dt})")

    exchange = get_exchange(exchange_name)
    since_ms = int(since_dt.timestamp() * 1000)
    until_ms = int(until_dt.timestamp() * 1000)
    all_data: list[list] = []

    logger.info(f"{exchange_name} | {symbol} | {timeframe} | {since_dt.date()} ‚Üí {until_dt.date()}")

    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∞–≥ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
    try:
        delta = exchange.parse_timeframe(timeframe)
    except Exception:
        delta = {"1d": 86400, "1h": 3600, "5m": 300}.get(timeframe, 3600)

    max_attempts = 5
    attempt = 0

    total_est = int((until_dt - since_dt).total_seconds() / delta)
    pbar = tqdm(total=total_est, desc=f"{exchange_name} {symbol} {timeframe}", ncols=90)

    while since_ms < until_ms:
        try:
            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since_ms, limit)
            attempt = 0
        except Exception as e:
            attempt += 1
            if attempt >= max_attempts:
                logger.error(f"üö® –ü—Ä–µ–≤—ã—à–µ–Ω–æ —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫ ({max_attempts}). –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏.")
                break
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ({attempt}/{max_attempts}): {e}")
            time.sleep(5 * attempt)
            continue

        if not ohlcv:
            logger.info("–ë–∏—Ä–∂–∞ –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç ‚Äî –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –¥–∏–∞–ø–∞–∑–æ–Ω–∞.")
            break

        if all_data and ohlcv[0][0] <= all_data[-1][0]:
            ohlcv = [row for row in ohlcv if row[0] > all_data[-1][0]]

        all_data.extend(ohlcv)
        pbar.update(len(ohlcv))

        since_ms = ohlcv[-1][0] + delta * 1000
        time.sleep(getattr(exchange, "rateLimit", 500) / 1000)

        if len(all_data) % 10000 < limit:
            tmp_df = pd.DataFrame(
                all_data, columns=["timestamp", "open", "high", "low", "close", "volume"]
            )
            tmp_path = (
                RAW_DIR / f"autosave_{exchange_name}_{symbol.replace('/', '')}_{timeframe}.parquet"
            )
            tmp_df.to_parquet(tmp_path)
            logger.info(f"üíæ Autosave: {len(tmp_df)} —Å—Ç—Ä–æ–∫ ‚Üí {tmp_path}")

    pbar.close()

    if not all_data:
        logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ—Ä—å —Å–∏–º–≤–æ–ª, —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏–ª–∏ –¥–∞—Ç—ã.")
        return pd.DataFrame(columns=["open", "high", "low", "close", "volume"])

    df = pd.DataFrame(all_data, columns=["timestamp", "open", "high", "low", "close", "volume"])
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
    df = df.drop_duplicates(subset="timestamp").sort_values("timestamp").set_index("timestamp")

    mask = (df.index >= since_dt) & (df.index <= until_dt)
    df = df.loc[mask].astype(float)

    logger.success(
        f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫: "
        f"{df.index.min().date()} ‚Üí {df.index.max().date()} "
        f"(–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ {until_dt.date()})"
    )
    return df


# ---------------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet ----------------


def save_parquet(
    df: pd.DataFrame,
    exchange: str,
    symbol: str,
    tf: str,
    start: dt.datetime,
    end: dt.datetime,
    out_dir: Path = RAW_DIR,
) -> Path:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ Parquet –ø–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É."""
    if df.empty:
        logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π DataFrame ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ.")
        return Path()

    symbol_clean = symbol.replace("/", "")
    filename = f"{exchange}__{symbol_clean}__{tf}__{start.date()}__{end.date()}.parquet"
    path = out_dir / filename

    df = df.copy()
    df.index = pd.to_datetime(df.index, utc=True)
    df = df.sort_index()

    df.to_parquet(path, index=True)
    logger.success(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –≤ {path}")
    return path


# ---------------- –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ ----------------


def collect_and_save(
    exchange_name: str,
    symbol: str,
    timeframes: list[str],
    start: str | dt.datetime,
    end: str | dt.datetime,
) -> None:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º."""
    start_dt = to_utc_datetime(start)
    end_dt = to_utc_datetime(end)

    for tf in timeframes:
        df = fetch_ohlcv(exchange_name, symbol, tf, start_dt, end_dt)
        missing_pct = qc_summary(df, tf)
        parquet_path = save_parquet(df, exchange_name, symbol, tf, start_dt, end_dt)
        if parquet_path.exists():
            write_metadata(
                parquet_path,
                exchange_name,
                symbol,
                tf,
                start_dt,
                end_dt,
                len(df),
                missing_pct,
            )
        logger.info(f"QC –∑–∞–≤–µ—Ä—à—ë–Ω: {missing_pct:.2f}% –ø—Ä–æ–ø—É—Å–∫–æ–≤ –¥–ª—è {tf}")


# ---------------- CLI-—Ç–µ—Å—Ç ----------------

if __name__ == "__main__":
    exch = "binance"
    sym = "BTC/USDT"
    tfs = ["5m"]
    start = "2025-04-30"
    end = "2025-10-01"

    collect_and_save(exch, sym, tfs, start, end)
